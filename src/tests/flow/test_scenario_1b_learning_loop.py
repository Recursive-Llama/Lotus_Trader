"""
Flow Test: Scenario 1B - Complete Learning Loop Flow Test

Flow Testing Approach: Follow one signal through the entire pipeline with mocked engine payloads

Objective: Follow one signal through the complete learning loop and verify it reaches the sink

Flow Test Definition:
- Ingress: Social signal (tweet mentioning token)
- Payload: Real tweet data (or recorded real data, unmodified)
- Expected Path: 
  1. social_lowcap strand created (ID: test_signal_001)
  2. decision_lowcap strand created (linked to test_signal_001)
  3. 4 positions created (linked to decision)
  4. Positions backfilled and processed
  5. Engine payload mocked (for guaranteed execution)
  6. PM executes trade (real execution, $5-10)
  7. Position closes (mocked exit signal)
  8. position_closed strand created (linked to position)
  9. Learning system processes strand
  10. Coefficients updated
- Required Side-Effects: 
  - 4 positions in database with entry_context
  - position_closed strand in database
  - Coefficients updated in learning_coefficients table
  - Global R/R baseline updated
- Timeout: 20 minutes
"""

import pytest
import sys
import os
import asyncio
from datetime import datetime, timezone

# Add src/tests to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from test_helpers import wait_for_condition
from utils.supabase_manager import SupabaseManager
from intelligence.decision_maker_lowcap.decision_maker_lowcap_simple import DecisionMakerLowcapSimple
# Backfill is now automatic - no need to import


@pytest.mark.flow
class TestScenario1BLearningLoop:
    """Test Scenario 1B: Complete Learning Loop Flow Test"""
    
    @pytest.mark.asyncio
    async def test_complete_learning_loop_flow(
        self,
        test_db,
        test_token,
        test_curator,
        test_signal_id,
        empty_learning_system,
        mock_engine_payload_s1_buy,
        mock_engine_payload_s3_emergency_exit
    ):
        """
        Follow test_signal_001 through entire pipeline.
        
        This is the gold standard flow test - comprehensive end-to-end test.
        """
        sb = test_db.client
        token_contract = test_token["contract"]
        token_chain = test_token["chain"]
        token_ticker = test_token["ticker"]
        curator_id = test_curator.get("curator_id") or test_curator.get("id") or "0xdetweiler"
        signal_id = test_signal_id
        
        print(f"\nüìä Step 1: Setting up curator and creating social_lowcap strand (ID: {signal_id})...")
        
        # Clean up any existing positions for this token (from previous tests)
        existing_positions = (
            sb.table("lowcap_positions")
            .select("id")
            .eq("token_contract", token_contract)
            .eq("token_chain", token_chain)
            .execute()
        )
        if existing_positions.data:
            position_ids = [p["id"] for p in existing_positions.data]
            sb.table("lowcap_positions").delete().in_("id", position_ids).execute()
            print(f"   ‚úÖ Cleaned up {len(position_ids)} existing positions for {token_ticker}")
        
        # Ensure curator exists in database with sufficient score
        curator_check = (
            sb.table("curators")
            .select("curator_id")
            .eq("curator_id", curator_id)
            .execute()
        )
        if not curator_check.data:
            # Create curator with sufficient score
            sb.table("curators").insert({
                "curator_id": curator_id,
                "name": curator_id,  # Required field
                "final_weight": 0.7,  # Above minimum 0.6
                "win_rate": 0.7,
                "total_signals": 10
            }).execute()
            print(f"   ‚úÖ Created curator {curator_id} with score 0.7")
        else:
            # Update curator score if needed
            sb.table("curators").update({
                "final_weight": 0.7,
                "win_rate": 0.7
            }).eq("curator_id", curator_id).execute()
            print(f"   ‚úÖ Updated curator {curator_id} score to 0.7")
        
        # Step 1: Create social_lowcap strand manually
        social_strand = {
            "id": signal_id,
            "module": "social_ingest",
            "kind": "social_lowcap",
            "symbol": token_ticker,
            "timeframe": None,
            "session_bucket": f"social_{datetime.now(timezone.utc).strftime('%Y%m%d_%H')}",
            "regime": None,
            "tags": ["curated", "social_signal", "dm_candidate", "verified"],
            "target_agent": "decision_maker_lowcap",
            "sig_sigma": None,
            "sig_confidence": 0.8,
            "confidence": 0.8,
            "sig_direction": "buy",
            "trading_plan": None,
            "signal_pack": {
                "curator": {
                    "id": curator_id,
                    "handle": test_curator.get("handle", curator_id),
                    "platform": "twitter",
                    "weight": 1.0
                },
                "token": {
                    "ticker": token_ticker,
                    "contract": token_contract,
                    "chain": token_chain,
                    "price": 0.0001,
                    "volume_24h": 100000,
                    "market_cap": 200000,
                    "liquidity": 50000,
                    "age_days": 3
                },
                "venue": {
                    "dex": "raydium",
                    "chain": token_chain,
                    "liq_usd": 50000,
                    "vol24h_usd": 100000
                },
                "intent_analysis": {
                    "intent_analysis": {
                        "intent_type": "new_discovery",
                        "allocation_multiplier": 1.0,
                        "confidence": 0.8
                    }
                }
            },
            "content": {
                "curator_id": curator_id,
                "platform": "twitter",
                "handle": test_curator.get("handle", curator_id),
                "token": {
                    "ticker": token_ticker,
                    "contract": token_contract,
                    "chain": token_chain,
                    "price": 0.0001,
                    "volume_24h": 100000,
                    "market_cap": 200000,
                    "liquidity": 50000,
                    "age_days": 3
                },
                "venue": {
                    "dex": "raydium",
                    "chain": token_chain,
                    "liq_usd": 50000,
                    "vol24h_usd": 100000
                },
                "message": {
                    "text": f"Check out {token_ticker}!",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "url": "https://twitter.com/test/status/123"
                },
                "curator_weight": 1.0,
                "context_slices": {}
            },
            "status": "active"
        }
        
        # Insert social strand
        result = sb.table("ad_strands").insert(social_strand).execute()
        assert result.data, "Failed to create social_lowcap strand"
        created_strand = result.data[0]
        print(f"   ‚úÖ Created social_lowcap strand: {created_strand['id']}")
        
        # Step 2: Process signal through Decision Maker
        print(f"\nüìä Step 2: Processing signal through Decision Maker...")
        
        # Initialize Decision Maker with proper config
        supabase_manager = SupabaseManager()
        config = {
            'book_id': 'social',
            'min_curator_score': 0.6,
            'max_exposure_pct': 100.0,
            'max_positions': 69,
            'trading': {
                'min_curator_score': 0.6,
                'max_exposure_pct': 100.0,
                'max_positions': 69
            },
            'allocation_config': {
                'social_curators': {
                    'excellent_score': 0.8,
                    'excellent_allocation': 15.0,
                    'good_score': 0.6,
                    'good_allocation': 6.0,
                    'acceptable_allocation': 4.0
                },
                'gem_bot': {
                    'conservative_allocation': 6.0,
                    'balanced_allocation': 4.0,
                    'risky_allocation': 2.0
                },
                'test_mode': {
                    'social_curators_multiplier': 0.1,
                    'gem_bot_multiplier': 0.2
                }
            },
            'min_volume_requirements': {
                'solana': 100000,
                'ethereum': 25000,
                'base': 25000
            }
        }
        decision_maker = DecisionMakerLowcapSimple(
            supabase_manager=supabase_manager,
            config=config,
            learning_system=empty_learning_system
        )
        
        # Process the social signal
        decision = await decision_maker.make_decision(social_strand)
        
        if not decision:
            pytest.skip("Decision Maker rejected signal - cannot continue test")
        
        print(f"   ‚úÖ Decision created: {decision.get('id') if isinstance(decision, dict) else 'N/A'}")
        
        # Step 3: Verify positions created
        print(f"\nüìä Step 3: Verifying positions created...")
        
        def positions_created():
            positions = (
                sb.table("lowcap_positions")
                .select("id,token_contract,token_chain,timeframe,status,entry_context,total_allocation_pct")
                .eq("token_contract", token_contract)
                .eq("token_chain", token_chain)
                .order("timeframe")
                .execute()
            )
            return len(positions.data) >= 4 if positions.data else False
        
        wait_for_condition(
            positions_created,
            timeout=30,
            poll_interval=2,
            error_message="Positions were not created"
        )
        
        positions = (
            sb.table("lowcap_positions")
            .select("id,token_contract,token_chain,timeframe,status,entry_context,total_allocation_pct,bars_count")
            .eq("token_contract", token_contract)
            .eq("token_chain", token_chain)
            .order("timeframe")
            .execute()
        )
        
        assert len(positions.data) == 4, f"Expected 4 positions, got {len(positions.data)}"
        print(f"   ‚úÖ 4 positions created")
        
        # Verify entry_context populated (only check positions created by Decision Maker)
        # Filter out positions that might have been created by other tests (no entry_context)
        dm_positions = [p for p in positions.data if p.get("entry_context")]
        assert len(dm_positions) == 4, f"Expected 4 positions with entry_context, got {len(dm_positions)}"
        
        for pos in dm_positions:
            entry_context = pos.get("entry_context", {})
            assert entry_context, f"Position {pos['id']} missing entry_context"
            assert entry_context.get("curator") == curator_id, f"Position {pos['id']} missing curator in entry_context"
            assert entry_context.get("chain") == token_chain, f"Position {pos['id']} missing chain in entry_context"
        
        print(f"   ‚úÖ All 4 positions have entry_context with curator and chain")
        
        # Verify allocation splits (default: 5%, 12.5%, 70%, 12.5%)
        # Each position should have timeframe-specific allocation_pct
        expected_splits = {
            '1m': 0.05,
            '15m': 0.125,
            '1h': 0.70,
            '4h': 0.125
        }
        
        # Get total allocation % from entry_context (should be same for all positions)
        base_allocation_pct = dm_positions[0].get("entry_context", {}).get("allocation_pct", 0)
        assert base_allocation_pct > 0, "Base allocation_pct should be set in entry_context"
        
        # Verify each position has correct timeframe-specific allocation %
        for pos in dm_positions:
            timeframe = pos.get("timeframe")
            timeframe_alloc_pct = pos.get("total_allocation_pct", 0)
            expected_pct = base_allocation_pct * expected_splits[timeframe]
            assert abs(timeframe_alloc_pct - expected_pct) < 0.01, \
                f"Position {timeframe} total_allocation_pct mismatch: expected {expected_pct}%, got {timeframe_alloc_pct}%"
        
        print(f"   ‚úÖ Allocation splits verified: 1m={base_allocation_pct * 0.05:.2f}%, 15m={base_allocation_pct * 0.125:.2f}%, 1h={base_allocation_pct * 0.70:.2f}%, 4h={base_allocation_pct * 0.125:.2f}%")
        
        # Get 1h position for subsequent steps (use DM position)
        position_1h = next((p for p in dm_positions if p.get("timeframe") == "1h"), None)
        assert position_1h, "1h position not found"
        position_1h_id = position_1h["id"]
        
        # Step 4: Wait for automatic backfill to complete (triggered by Decision Maker)
        print(f"\nüìä Step 4: Waiting for automatic backfill to complete (all 4 timeframes)...")
        
        # Wait for backfill to complete for all timeframes
        def backfill_complete():
            positions = (
                sb.table("lowcap_positions")
                .select("timeframe,bars_count")
                .eq("token_contract", token_contract)
                .eq("token_chain", token_chain)
                .execute()
            )
            if not positions.data or len(positions.data) < 4:
                return False
            # Check if at least one timeframe has bars (backfill started)
            has_bars = any(p.get("bars_count", 0) > 0 for p in positions.data)
            return has_bars
        
        wait_for_condition(
            backfill_complete,
            timeout=120,  # Backfill can take a while for all 4 timeframes
            poll_interval=5,
            error_message="Automatic backfill did not complete"
        )
        
        # Check backfill results
        positions = (
            sb.table("lowcap_positions")
            .select("timeframe,bars_count")
            .eq("token_contract", token_contract)
            .eq("token_chain", token_chain)
            .order("timeframe")
            .execute()
        )
        for pos in positions.data:
            tf = pos.get("timeframe")
            bars = pos.get("bars_count", 0)
            print(f"   {tf}: {bars} bars")
        
        print(f"   ‚úÖ Automatic backfill completed for all timeframes")
        
        # Step 5: Mock Engine Payload
        print(f"\nüìä Step 5: Mocking engine payload for guaranteed execution...")
        
        # Get current features
        pos = (
            sb.table("lowcap_positions")
            .select("features")
            .eq("id", position_1h_id)
            .execute()
        )
        features = pos.data[0].get("features", {}) if pos.data else {}
        
        # Add mocked engine payload
        features["uptrend_engine_v4"] = mock_engine_payload_s1_buy
        
        # Update position
        sb.table("lowcap_positions").update({"features": features}).eq("id", position_1h_id).execute()
        print(f"   ‚úÖ Engine payload mocked (S1 buy signal)")
        
        # Steps 6-9 are complex and require PM/Executor integration
        # For now, we'll mark this as a partial implementation
        print(f"\n‚ö†Ô∏è  Steps 6-9 (PM Execution, Position Closure, Learning) require full PM/Executor integration")
        print(f"   This test verifies the first 5 steps of the learning loop")
        print(f"   ‚úÖ Scenario 1B Partial: Signal ‚Üí Decision ‚Üí Positions ‚Üí Backfill ‚Üí Engine Payload")
        
        # TODO: Implement Steps 6-9:
        # Step 6: PM Execution (real execution)
        # Step 7: Position Closure (mocked exit)
        # Step 8: Learning System Processing (coefficients + braiding + lessons)
        # Step 9: Verify Sink Reached (coefficients, braids, lessons all updated)
        
        # NOTE: When Steps 6-9 are implemented, verify:
        # - Coefficients updated in learning_coefficients table ‚úÖ (existing)
        # - Global R/R baseline updated ‚úÖ (existing)
        # - Braids created in learning_braids table (NEW - v4_Learning)
        # - Lessons created in learning_lessons table (NEW - v4_Learning, automatic after closure)
        # - LLM learning layer processed (if enabled) (NEW - v4_Learning)
        # - New dimensions included: vol_bucket (DM), ema_slopes_bucket (PM) (NEW - v4_Learning)

