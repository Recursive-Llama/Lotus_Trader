#!/usr/bin/env python3
"""
Deep Comprehensive Learning System Test
Tests ALL components with CORRECT imports and then dives much deeper
"""

import sys
import os
import asyncio
import json
import time
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional
import traceback

# Add project root to path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'Modules', 'Alpha_Detector', 'src'))

class DeepComprehensiveValidator:
    def __init__(self):
        self.failures = []
        self.successes = []
        self.test_strands = []
        self.test_braids = []
        self.performance_metrics = {}
        
    def log_failure(self, component: str, error: str):
        """Log a real failure"""
        self.failures.append(f"{component}: {error}")
        print(f"‚ùå {component}: {error}")
        
    def log_success(self, component: str, details: str = ""):
        """Log a real success"""
        self.successes.append(f"{component}: {details}")
        print(f"‚úÖ {component}: {details}")
    
    def create_comprehensive_test_data(self):
        """Create comprehensive test data for deep testing"""
        self.test_strands = [
            # Pattern strands
            {
                'kind': 'pattern',
                'content': 'RSI divergence detected on 1H timeframe',
                'metadata': {
                    'rsi': 30,
                    'timeframe': '1H',
                    'confidence': 0.8,
                    'pattern_type': 'rsi_divergence',
                    'accuracy': 0.8,
                    'precision': 0.75,
                    'stability': 0.8,
                    'symbol': 'BTC',
                    'price': 50000.0
                },
                'source': 'raw_data_intelligence',
                'created_at': datetime.now(timezone.utc).isoformat()
            },
            {
                'kind': 'pattern',
                'content': 'Volume spike with price drop',
                'metadata': {
                    'volume': 1500000,
                    'price_change': -0.05,
                    'confidence': 0.7,
                    'pattern_type': 'volume_spike',
                    'accuracy': 0.7,
                    'precision': 0.6,
                    'stability': 0.7,
                    'symbol': 'ETH',
                    'price': 3000.0
                },
                'source': 'raw_data_intelligence',
                'created_at': datetime.now(timezone.utc).isoformat()
            },
            # Prediction review strands
            {
                'kind': 'prediction_review',
                'content': 'Prediction accuracy: 85% for BTC price movement',
                'metadata': {
                    'prediction_id': 'pred_001',
                    'accuracy': 0.85,
                    'confidence': 0.8,
                    'outcome': 'success',
                    'symbol': 'BTC',
                    'predicted_direction': 'up',
                    'actual_direction': 'up',
                    'price_target': 52000,
                    'actual_price': 51500
                },
                'source': 'central_intelligence_layer',
                'created_at': datetime.now(timezone.utc).isoformat()
            },
            {
                'kind': 'prediction_review',
                'content': 'Prediction failed: ETH price prediction was incorrect',
                'metadata': {
                    'prediction_id': 'pred_002',
                    'accuracy': 0.3,
                    'confidence': 0.6,
                    'outcome': 'failure',
                    'symbol': 'ETH',
                    'predicted_direction': 'up',
                    'actual_direction': 'down',
                    'price_target': 3200,
                    'actual_price': 2800
                },
                'source': 'central_intelligence_layer',
                'created_at': datetime.now(timezone.utc).isoformat()
            },
            # Trade outcome strands
            {
                'kind': 'trade_outcome',
                'content': 'Trade executed: BUY 100 shares at $50, sold at $55',
                'metadata': {
                    'action': 'BUY',
                    'quantity': 100,
                    'entry_price': 50,
                    'exit_price': 55,
                    'profit': 500,
                    'outcome': 'success',
                    'symbol': 'BTC',
                    'execution_time': 120,
                    'slippage': 0.001
                },
                'source': 'trading_execution',
                'created_at': datetime.now(timezone.utc).isoformat()
            },
            {
                'kind': 'trade_outcome',
                'content': 'Trade executed: SELL 50 shares at $45, sold at $40',
                'metadata': {
                    'action': 'SELL',
                    'quantity': 50,
                    'entry_price': 45,
                    'exit_price': 40,
                    'profit': 250,
                    'outcome': 'success',
                    'symbol': 'ETH',
                    'execution_time': 90,
                    'slippage': 0.002
                },
                'source': 'trading_execution',
                'created_at': datetime.now(timezone.utc).isoformat()
            },
            # Trading decision strands
            {
                'kind': 'trading_decision',
                'content': 'Decision: Hold position due to high volatility',
                'metadata': {
                    'decision_type': 'hold',
                    'reasoning': 'high_volatility',
                    'confidence': 0.7,
                    'risk_level': 'medium',
                    'symbol': 'BTC',
                    'market_conditions': 'volatile'
                },
                'source': 'decision_maker',
                'created_at': datetime.now(timezone.utc).isoformat()
            }
        ]
        
        self.test_braids = [
            {
                'id': 'braid_001',
                'content': 'High-confidence RSI divergence pattern with 85% accuracy',
                'strand_types': ['pattern'],
                'resonance_score': 0.85,
                'level': 2,
                'created_at': datetime.now(timezone.utc).isoformat(),
                'metadata': {
                    'pattern_quality': 'high',
                    'timeframe_consistency': 0.9,
                    'market_conditions': 'trending'
                }
            },
            {
                'id': 'braid_002',
                'content': 'Successful trading strategy with consistent profits',
                'strand_types': ['prediction_review', 'trade_outcome'],
                'resonance_score': 0.9,
                'level': 3,
                'created_at': datetime.now(timezone.utc).isoformat(),
                'metadata': {
                    'profit_consistency': 0.85,
                    'risk_adjusted_return': 0.78,
                    'win_rate': 0.8
                }
            }
        ]
    
    async def test_mathematical_resonance_deep(self):
        """Test MathematicalResonanceEngine with DEEP comprehensive scenarios"""
        print("\nüßÆ TESTING MATHEMATICAL RESONANCE ENGINE (DEEP)")
        print("="*70)
        
        try:
            # Test 1: Import and initialize
            print("Testing MathematicalResonanceEngine import...")
            try:
                from mathematical_resonance_engine import MathematicalResonanceEngine
                engine = MathematicalResonanceEngine()
                self.log_success("MathematicalResonanceEngine import and initialization")
            except Exception as e:
                self.log_failure("MathematicalResonanceEngine import", str(e))
                return False
            
            # Test 2: œÜ (Fractal Self-Similarity) - Multiple scenarios
            print("Testing œÜ (Fractal Self-Similarity) with multiple scenarios...")
            try:
                test_scenarios = [
                    {
                        'name': 'High Quality Pattern',
                        'pattern_data': {'accuracy': 0.9, 'precision': 0.85, 'stability': 0.9},
                        'timeframes': ['1H', '4H', '1D', '1W'],
                        'expected_range': (0.7, 1.0)
                    },
                    {
                        'name': 'Medium Quality Pattern',
                        'pattern_data': {'accuracy': 0.7, 'precision': 0.6, 'stability': 0.7},
                        'timeframes': ['1H', '4H', '1D'],
                        'expected_range': (0.3, 0.8)
                    },
                    {
                        'name': 'Low Quality Pattern',
                        'pattern_data': {'accuracy': 0.4, 'precision': 0.3, 'stability': 0.4},
                        'timeframes': ['1H'],
                        'expected_range': (0.0, 0.5)
                    },
                    {
                        'name': 'Edge Case - Perfect Pattern',
                        'pattern_data': {'accuracy': 1.0, 'precision': 1.0, 'stability': 1.0},
                        'timeframes': ['1H', '4H', '1D', '1W', '1M'],
                        'expected_range': (0.8, 1.0)
                    }
                ]
                
                for scenario in test_scenarios:
                    phi = engine.calculate_phi(scenario['pattern_data'], scenario['timeframes'])
                    
                    if scenario['expected_range'][0] <= phi <= scenario['expected_range'][1]:
                        self.log_success(f"œÜ calculation: {scenario['name']}", f"œÜ = {phi:.3f}")
                    else:
                        self.log_failure(f"œÜ calculation: {scenario['name']}", f"Invalid œÜ = {phi:.3f}")
                        return False
                        
            except Exception as e:
                self.log_failure("œÜ calculations", str(e))
                return False
            
            # Test 3: œÅ (Recursive Feedback) - Multiple scenarios
            print("Testing œÅ (Recursive Feedback) with multiple scenarios...")
            try:
                test_scenarios = [
                    {
                        'name': 'High Learning Success',
                        'learning_outcome': {
                            'pattern_performance_history': {'pattern1': [0.8, 0.85, 0.9, 0.95]},
                            'successful_braids_count': 10,
                            'learning_strength': 0.8
                        },
                        'expected_range': (0.6, 1.0)
                    },
                    {
                        'name': 'Medium Learning Success',
                        'learning_outcome': {
                            'pattern_performance_history': {'pattern1': [0.5, 0.6, 0.7]},
                            'successful_braids_count': 5,
                            'learning_strength': 0.5
                        },
                        'expected_range': (0.2, 0.7)
                    },
                    {
                        'name': 'Low Learning Success',
                        'learning_outcome': {
                            'pattern_performance_history': {'pattern1': [0.3, 0.4, 0.5]},
                            'successful_braids_count': 2,
                            'learning_strength': 0.3
                        },
                        'expected_range': (0.0, 0.5)
                    }
                ]
                
                for scenario in test_scenarios:
                    rho = engine.calculate_rho(scenario['learning_outcome'])
                    
                    if scenario['expected_range'][0] <= rho <= scenario['expected_range'][1]:
                        self.log_success(f"œÅ calculation: {scenario['name']}", f"œÅ = {rho:.3f}")
                    else:
                        self.log_failure(f"œÅ calculation: {scenario['name']}", f"Invalid œÅ = {rho:.3f}")
                        return False
                        
            except Exception as e:
                self.log_failure("œÅ calculations", str(e))
                return False
            
            # Test 4: Œ∏ (Global Field) - Multiple scenarios
            print("Testing Œ∏ (Global Field) with multiple scenarios...")
            try:
                test_scenarios = [
                    {
                        'name': 'High Quality Braids',
                        'all_braids': [
                            {'resonance_score': 0.9, 'accuracy': 0.9},
                            {'resonance_score': 0.85, 'accuracy': 0.85},
                            {'resonance_score': 0.8, 'accuracy': 0.8}
                        ],
                        'expected_range': (0.7, 1.0)
                    },
                    {
                        'name': 'Mixed Quality Braids',
                        'all_braids': [
                            {'resonance_score': 0.8, 'accuracy': 0.8},
                            {'resonance_score': 0.6, 'accuracy': 0.6},
                            {'resonance_score': 0.4, 'accuracy': 0.4}
                        ],
                        'expected_range': (0.3, 0.8)
                    },
                    {
                        'name': 'Low Quality Braids',
                        'all_braids': [
                            {'resonance_score': 0.3, 'accuracy': 0.3},
                            {'resonance_score': 0.4, 'accuracy': 0.4}
                        ],
                        'expected_range': (0.0, 0.5)
                    },
                    {
                        'name': 'Empty Braids',
                        'all_braids': [],
                        'expected_range': (0.0, 1.0)
                    }
                ]
                
                for scenario in test_scenarios:
                    theta = engine.calculate_theta(scenario['all_braids'])
                    
                    if scenario['expected_range'][0] <= theta <= scenario['expected_range'][1]:
                        self.log_success(f"Œ∏ calculation: {scenario['name']}", f"Œ∏ = {theta:.3f}")
                    else:
                        self.log_failure(f"Œ∏ calculation: {scenario['name']}", f"Invalid Œ∏ = {theta:.3f}")
                        return False
                        
            except Exception as e:
                self.log_failure("Œ∏ calculations", str(e))
                return False
            
            # Test 5: œâ (Resonance Acceleration) - Multiple scenarios
            print("Testing œâ (Resonance Acceleration) with multiple scenarios...")
            try:
                test_scenarios = [
                    {
                        'name': 'High Theta, High Learning',
                        'global_theta': 0.9,
                        'learning_strength': 0.8,
                        'expected_range': (0.7, 1.0)
                    },
                    {
                        'name': 'Medium Theta, Medium Learning',
                        'global_theta': 0.6,
                        'learning_strength': 0.5,
                        'expected_range': (0.3, 0.8)
                    },
                    {
                        'name': 'Low Theta, Low Learning',
                        'global_theta': 0.3,
                        'learning_strength': 0.2,
                        'expected_range': (0.0, 0.5)
                    }
                ]
                
                for scenario in test_scenarios:
                    omega = engine.calculate_omega(scenario['global_theta'], scenario['learning_strength'])
                    
                    if scenario['expected_range'][0] <= omega <= scenario['expected_range'][1]:
                        self.log_success(f"œâ calculation: {scenario['name']}", f"œâ = {omega:.3f}")
                    else:
                        self.log_failure(f"œâ calculation: {scenario['name']}", f"Invalid œâ = {omega:.3f}")
                        return False
                        
            except Exception as e:
                self.log_failure("œâ calculations", str(e))
                return False
            
            # Test 6: S_i (Selection Score) - Multiple scenarios
            print("Testing S_i (Selection Score) with multiple scenarios...")
            try:
                test_scenarios = [
                    {
                        'name': 'High Quality Pattern',
                        'pattern_data': {
                            'accuracy': 0.9,
                            'precision': 0.85,
                            'stability': 0.9,
                            'cost': 0.05,
                            'orthogonality': 0.8
                        },
                        'expected_range': (0.7, 1.0)
                    },
                    {
                        'name': 'Medium Quality Pattern',
                        'pattern_data': {
                            'accuracy': 0.7,
                            'precision': 0.6,
                            'stability': 0.7,
                            'cost': 0.1,
                            'orthogonality': 0.6
                        },
                        'expected_range': (0.3, 0.8)
                    },
                    {
                        'name': 'Low Quality Pattern',
                        'pattern_data': {
                            'accuracy': 0.4,
                            'precision': 0.3,
                            'stability': 0.4,
                            'cost': 0.2,
                            'orthogonality': 0.3
                        },
                        'expected_range': (0.0, 0.5)
                    }
                ]
                
                for scenario in test_scenarios:
                    selection_score = engine.calculate_selection_score(scenario['pattern_data'])
                    
                    if hasattr(selection_score, 'total_score'):
                        score = selection_score.total_score
                    else:
                        score = selection_score
                    
                    if scenario['expected_range'][0] <= score <= scenario['expected_range'][1]:
                        self.log_success(f"S_i calculation: {scenario['name']}", f"S_i = {score:.3f}")
                    else:
                        self.log_failure(f"S_i calculation: {scenario['name']}", f"Invalid S_i = {score:.3f}")
                        return False
                        
            except Exception as e:
                self.log_failure("S_i calculations", str(e))
                return False
            
            # Test 7: Module Resonance - All modules
            print("Testing Module Resonance for all modules...")
            try:
                modules = ['cil', 'ctp', 'dm', 'td', 'rdi']
                
                for module in modules:
                    strand = self.test_strands[0]
                    module_resonance = engine.calculate_module_resonance(strand, module)
                    
                    if isinstance(module_resonance, dict):
                        self.log_success(f"Module resonance: {module.upper()}", f"Calculated: {module_resonance}")
                    else:
                        self.log_success(f"Module resonance: {module.upper()}", f"Result: {module_resonance}")
                        
            except Exception as e:
                self.log_failure("Module resonance", str(e))
                return False
            
            return True
            
        except Exception as e:
            self.log_failure("Mathematical Resonance Engine", f"Unexpected error: {e}")
            return False
    
    async def test_strand_processor_deep(self):
        """Test StrandProcessor with DEEP comprehensive scenarios"""
        print("\nüßµ TESTING STRAND PROCESSOR (DEEP)")
        print("="*70)
        
        try:
            # Test 1: Import and initialize
            print("Testing StrandProcessor import...")
            try:
                from strand_processor import StrandProcessor
                processor = StrandProcessor()
                self.log_success("StrandProcessor import and initialization")
            except Exception as e:
                self.log_failure("StrandProcessor import", str(e))
                return False
            
            # Test 2: All Strand Types Processing
            print("Testing all strand types processing...")
            try:
                for strand in self.test_strands:
                    config = processor.process_strand(strand)
                    
                    if config:
                        self.log_success(f"Strand processing: {strand['kind']}", f"Config: {config.learning_focus}")
                    else:
                        self.log_success(f"Strand processing: {strand['kind']}", "No config (unsupported type)")
                        
            except Exception as e:
                self.log_failure("Strand processing", str(e))
                return False
            
            # Test 3: Learning Config for All Types
            print("Testing learning config for all supported types...")
            try:
                from strand_processor import StrandType
                
                supported_types = [
                    StrandType.PATTERN,
                    StrandType.PREDICTION_REVIEW,
                    StrandType.CONDITIONAL_TRADING_PLAN,
                    StrandType.TRADE_OUTCOME,
                    StrandType.TRADING_DECISION,
                    StrandType.PORTFOLIO_OUTCOME,
                    StrandType.EXECUTION_OUTCOME
                ]
                
                for strand_type in supported_types:
                    config = processor.get_learning_config(strand_type)
                    
                    if config:
                        self.log_success(f"Learning config: {strand_type.value}", f"Focus: {config.learning_focus}")
                    else:
                        self.log_failure(f"Learning config: {strand_type.value}", "No config found")
                        return False
                        
            except Exception as e:
                self.log_failure("Learning config retrieval", str(e))
                return False
            
            # Test 4: Learning Support for All Types
            print("Testing learning support for all types...")
            try:
                all_types = ['pattern', 'prediction_review', 'conditional_trading_plan', 'trade_outcome', 
                           'trading_decision', 'portfolio_outcome', 'execution_outcome', 'unsupported_type']
                
                for strand_type in all_types:
                    is_supported = processor.is_learning_supported(strand_type)
                    
                    if is_supported:
                        self.log_success(f"Learning support: {strand_type}", "Supported")
                    else:
                        self.log_success(f"Learning support: {strand_type}", "Not supported")
                        
            except Exception as e:
                self.log_failure("Learning support check", str(e))
                return False
            
            # Test 5: Supported Strand Types
            print("Testing supported strand types...")
            try:
                supported_types = processor.get_supported_strand_types()
                
                if supported_types and len(supported_types) > 0:
                    self.log_success("Supported strand types", f"Found {len(supported_types)} types: {supported_types}")
                else:
                    self.log_failure("Supported strand types", "No types found")
                    return False
                    
            except Exception as e:
                self.log_failure("Supported strand types", str(e))
                return False
            
            return True
            
        except Exception as e:
            self.log_failure("Strand Processor", f"Unexpected error: {e}")
            return False
    
    async def test_context_injection_deep(self):
        """Test ContextInjectionEngine with DEEP comprehensive scenarios"""
        print("\nüíâ TESTING CONTEXT INJECTION ENGINE (DEEP)")
        print("="*70)
        
        try:
            # Test 1: Import and initialize
            print("Testing ContextInjectionEngine import...")
            try:
                from context_injection_engine import ContextInjectionEngine
                self.log_success("ContextInjectionEngine import")
            except Exception as e:
                self.log_failure("ContextInjectionEngine import", str(e))
                return False
            
            # Test 2: Initialize with comprehensive mock
            print("Testing ContextInjectionEngine initialization...")
            try:
                class ComprehensiveMockSupabaseManager:
                    def __init__(self):
                        self.test_braids = self.test_braids
                    
                    async def get_braids_for_module(self, module, strand_types):
                        return self.test_braids
                    
                    async def get_braids_by_strand_types(self, strand_types):
                        return [b for b in self.test_braids if any(st in b.get('strand_types', []) for st in strand_types)]
                    
                    async def get_braids_by_resonance_score(self, min_score):
                        return [b for b in self.test_braids if b.get('resonance_score', 0) >= min_score]
                
                context_engine = ContextInjectionEngine(ComprehensiveMockSupabaseManager())
                self.log_success("ContextInjectionEngine initialization with comprehensive mocks")
                
            except Exception as e:
                self.log_failure("ContextInjectionEngine initialization", str(e))
                return False
            
            # Test 3: Context Retrieval for All Modules
            print("Testing context retrieval for all modules...")
            try:
                modules = ['cil', 'ctp', 'dm', 'td', 'rdi']
                
                for module in modules:
                    context = await context_engine.get_context_for_module(module, {'strand_types': ['pattern', 'prediction_review']})
                    
                    if context:
                        self.log_success(f"Context retrieval: {module.upper()}", f"Generated context with {len(context)} items")
                    else:
                        self.log_success(f"Context retrieval: {module.upper()}", "Generated empty context")
                        
            except Exception as e:
                self.log_failure("Context retrieval for all modules", str(e))
                return False
            
            # Test 4: Context Statistics
            print("Testing context statistics...")
            try:
                stats = await context_engine.get_context_statistics()
                
                if stats:
                    self.log_success("Context statistics", f"Retrieved comprehensive stats: {stats}")
                else:
                    self.log_success("Context statistics", "No statistics available")
                    
            except Exception as e:
                self.log_failure("Context statistics", str(e))
                return False
            
            return True
            
        except Exception as e:
            self.log_failure("Context Injection Engine", f"Unexpected error: {e}")
            return False
    
    async def test_learning_pipeline_deep(self):
        """Test LearningPipeline with DEEP comprehensive scenarios"""
        print("\nüîÑ TESTING LEARNING PIPELINE (DEEP)")
        print("="*70)
        
        try:
            # Test 1: Import
            print("Testing LearningPipeline import...")
            try:
                from learning_pipeline import LearningPipeline
                self.log_success("LearningPipeline import")
            except Exception as e:
                self.log_failure("LearningPipeline import", str(e))
                return False
            
            # Test 2: Initialize with comprehensive mock dependencies
            print("Testing LearningPipeline initialization...")
            try:
                class ComprehensiveMockSupabaseManager:
                    def __init__(self):
                        pass
                    async def get_strands(self, **kwargs):
                        return self.test_strands
                    async def insert_braid(self, braid):
                        return {'id': f'braid_{len(self.test_braids) + 1}'}
                    async def get_braids(self, **kwargs):
                        return self.test_braids
                    async def update_strand(self, strand_id, updates):
                        return True
                
                class ComprehensiveMockLLMClient:
                    def __init__(self):
                        pass
                    async def generate_completion(self, **kwargs):
                        return {'content': f'Generated braid content for {kwargs.get("prompt", "unknown")[:50]}...'}
                
                class ComprehensiveMockPromptManager:
                    def __init__(self):
                        pass
                    def get_prompt(self, template_name):
                        return {'content': f'Prompt template for {template_name}'}
                
                pipeline = LearningPipeline(
                    ComprehensiveMockSupabaseManager(),
                    ComprehensiveMockLLMClient(),
                    ComprehensiveMockPromptManager()
                )
                
                self.log_success("LearningPipeline initialization with comprehensive mocks")
                
            except Exception as e:
                self.log_failure("LearningPipeline initialization", str(e))
                return False
            
            return True
            
        except Exception as e:
            self.log_failure("Learning Pipeline", f"Unexpected error: {e}")
            return False
    
    async def test_module_specific_scoring_deep(self):
        """Test ModuleSpecificScoring with DEEP comprehensive scenarios"""
        print("\nüìä TESTING MODULE SPECIFIC SCORING (DEEP)")
        print("="*70)
        
        try:
            # Test 1: Import and initialize
            print("Testing ModuleSpecificScoring import...")
            try:
                from module_specific_scoring import ModuleSpecificScoring
                scorer = ModuleSpecificScoring()
                self.log_success("ModuleSpecificScoring import and initialization")
            except Exception as e:
                self.log_failure("ModuleSpecificScoring import", str(e))
                return False
            
            # Test 2: Module Score Calculation for All Modules
            print("Testing module score calculation for all modules...")
            try:
                modules = ['cil', 'ctp', 'dm', 'td', 'rdi']
                
                for module in modules:
                    for strand in self.test_strands:
                        score = scorer.calculate_module_score(module, {
                            'strands': [strand],
                            'resonance_context': {'phi': 0.8, 'rho': 0.7, 'theta': 0.9, 'omega': 0.6}
                        })
                        
                        if score is not None:
                            self.log_success(f"Module score: {module.upper()} for {strand['kind']}", f"Score: {score:.3f}")
                        else:
                            self.log_success(f"Module score: {module.upper()} for {strand['kind']}", "No score calculated")
                        
            except Exception as e:
                self.log_failure("Module score calculation for all modules", str(e))
                return False
            
            # Test 3: Different Resonance Contexts
            print("Testing different resonance contexts...")
            try:
                resonance_contexts = [
                    {'phi': 0.9, 'rho': 0.8, 'theta': 0.95, 'omega': 0.7},
                    {'phi': 0.7, 'rho': 0.6, 'theta': 0.8, 'omega': 0.5},
                    {'phi': 0.5, 'rho': 0.4, 'theta': 0.6, 'omega': 0.3},
                    {'phi': 0.2, 'rho': 0.1, 'theta': 0.3, 'omega': 0.1}
                ]
                
                for i, context in enumerate(resonance_contexts):
                    score = scorer.calculate_module_score('cil', {
                        'strands': [self.test_strands[0]],
                        'resonance_context': context
                    })
                    
                    if score is not None:
                        self.log_success(f"Resonance context {i+1}", f"Score: {score:.3f}")
                    else:
                        self.log_success(f"Resonance context {i+1}", "No score calculated")
                        
            except Exception as e:
                self.log_failure("Different resonance contexts", str(e))
                return False
            
            return True
            
        except Exception as e:
            self.log_failure("Module Specific Scoring", f"Unexpected error: {e}")
            return False
    
    async def test_performance_metrics(self):
        """Test performance metrics and stress testing"""
        print("\n‚ö° TESTING PERFORMANCE METRICS")
        print("="*70)
        
        try:
            # Test 1: Mathematical Resonance Engine Performance
            print("Testing MathematicalResonanceEngine performance...")
            try:
                from mathematical_resonance_engine import MathematicalResonanceEngine
                engine = MathematicalResonanceEngine()
                
                # Test with large dataset
                start_time = time.time()
                
                for i in range(100):
                    pattern_data = {'accuracy': 0.8, 'precision': 0.75, 'stability': 0.8}
                    timeframes = ['1H', '4H', '1D']
                    phi = engine.calculate_phi(pattern_data, timeframes)
                
                end_time = time.time()
                duration = end_time - start_time
                
                self.performance_metrics['resonance_100_calculations'] = duration
                
                if duration < 1.0:  # Should complete in under 1 second
                    self.log_success("Resonance performance", f"100 calculations in {duration:.3f}s")
                else:
                    self.log_failure("Resonance performance", f"100 calculations took {duration:.3f}s (too slow)")
                    return False
                    
            except Exception as e:
                self.log_failure("Resonance performance", str(e))
                return False
            
            # Test 2: Strand Processor Performance
            print("Testing StrandProcessor performance...")
            try:
                from strand_processor import StrandProcessor
                processor = StrandProcessor()
                
                # Test with large dataset
                start_time = time.time()
                
                for i in range(100):
                    for strand in self.test_strands:
                        config = processor.process_strand(strand)
                
                end_time = time.time()
                duration = end_time - start_time
                
                self.performance_metrics['strand_processing_100_iterations'] = duration
                
                if duration < 2.0:  # Should complete in under 2 seconds
                    self.log_success("Strand processing performance", f"100 iterations in {duration:.3f}s")
                else:
                    self.log_failure("Strand processing performance", f"100 iterations took {duration:.3f}s (too slow)")
                    return False
                    
            except Exception as e:
                self.log_failure("Strand processing performance", str(e))
                return False
            
            return True
            
        except Exception as e:
            self.log_failure("Performance Metrics", f"Unexpected error: {e}")
            return False
    
    def print_deep_summary(self):
        """Print deep comprehensive summary"""
        print("\n" + "="*80)
        print("üîç DEEP COMPREHENSIVE LEARNING SYSTEM TEST RESULTS")
        print("="*80)
        
        print(f"\n‚úÖ SUCCESSES ({len(self.successes)}):")
        for success in self.successes:
            print(f"  {success}")
            
        print(f"\n‚ùå FAILURES ({len(self.failures)}):")
        for failure in self.failures:
            print(f"  {failure}")
        
        print(f"\n‚ö° PERFORMANCE METRICS:")
        for metric, value in self.performance_metrics.items():
            print(f"  {metric}: {value:.3f}s")
        
        print("\n" + "="*80)
        
        if len(self.failures) == 0:
            print("üéâ ALL LEARNING SYSTEM COMPONENTS ARE WORKING PERFECTLY")
            print("‚úÖ Mathematical Resonance Engine - All calculations working with multiple scenarios")
            print("‚úÖ Strand Processor - All processing working for all strand types")
            print("‚úÖ Context Injection Engine - All context delivery working for all modules")
            print("‚úÖ Learning Pipeline - All pipeline processing working")
            print("‚úÖ Module Specific Scoring - All scoring working for all modules")
            print("‚úÖ Performance - All components meet performance requirements")
            print("")
            print("üöÄ SYSTEM IS READY FOR PRODUCTION TESTING")
            print("üéØ Ready for real data flows and LLM integration testing")
        else:
            print("‚ö†Ô∏è  LEARNING SYSTEM STILL HAS ISSUES")
            print(f"   {len(self.failures)} components are failing")
            print("   Need to fix remaining issues before production")
            
        print("="*80)
        
        return len(self.failures) == 0

async def main():
    """Run deep comprehensive testing"""
    print("üöÄ DEEP COMPREHENSIVE LEARNING SYSTEM TESTING")
    print("Testing ALL components with DEEP scenarios and performance metrics")
    print("="*80)
    
    validator = DeepComprehensiveValidator()
    
    # Create comprehensive test data
    validator.create_comprehensive_test_data()
    
    # Test each component with deep scenarios
    tests = [
        ("Mathematical Resonance Engine (Deep)", validator.test_mathematical_resonance_deep),
        ("Strand Processor (Deep)", validator.test_strand_processor_deep),
        ("Context Injection Engine (Deep)", validator.test_context_injection_deep),
        ("Learning Pipeline (Deep)", validator.test_learning_pipeline_deep),
        ("Module Specific Scoring (Deep)", validator.test_module_specific_scoring_deep),
        ("Performance Metrics", validator.test_performance_metrics)
    ]
    
    for test_name, test_func in tests:
        try:
            print(f"\nüß™ Testing {test_name}...")
            await test_func()
        except Exception as e:
            validator.log_failure(test_name, f"Test crashed: {e}")
            traceback.print_exc()
    
    # Print deep results
    is_working = validator.print_deep_summary()
    
    return is_working

if __name__ == "__main__":
    asyncio.run(main())
